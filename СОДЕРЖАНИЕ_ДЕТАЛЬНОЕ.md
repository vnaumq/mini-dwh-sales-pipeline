# ДЕТАЛЬНОЕ СОДЕРЖАНИЕ КУРСОВОЙ РАБОТЫ

## "АВТОМАТИЗАЦИЯ ПРОЦЕССА СБОРА ДАННЫХ ИНТЕРНЕТ-МАГАЗИНА"

**Общий объем: 30-35 страниц**

---

## ВВЕДЕНИЕ

**Объем: 2-3 страницы**

- Актуальность темы исследования
- Цель курсовой работы
- Задачи исследования
- Объект и предмет исследования
- Методы исследования
- Практическая значимость работы
- Структура работы

---

## 1. ТЕОРЕТИЧЕСКИЕ ОСНОВЫ И ВЫБОР ТЕХНОЛОГИЙ

**Объем: 8-10 страниц**

### 1.1. Понятие и методы автоматизации сбора данных интернет-магазинов

- Определение автоматизации сбора данных
- Преимущества автоматизации перед ручным сбором
- Основные методы сбора данных:
  - Web Scraping и парсинг веб-страниц
  - Работа с REST API
  - Автоматизация браузера с использованием Selenium
- Проблемы и вызовы при автоматизации

### 1.2. ETL-процессы и оркестрация данных

- Понятие ETL-процессов (Extract, Transform, Load)
- Этапы ETL: извлечение, трансформация, загрузка
- Оркестрация ETL-процессов: планирование и мониторинг задач
- Хранилища данных: Data Warehouse, реляционные БД, объектные хранилища

### 1.3. Анализ платформы eggheads.solutions как источника данных

- Описание платформы eggheads.solutions
- Структура API и доступные эндпоинты
- Типы собираемых данных:
  - Иерархия категорий товаров (L1, L2, L3)
  - Аналитика продаж за 30 дней
  - Данные о товарах (subjects)
  - Коэффициенты сезонности
- Требования к аутентификации и ограничения API

### 1.4. Выбор технологического стека

#### 1.4.1. Обоснование выбора Apache Airflow

- Краткий обзор Apache Airflow: возможности и преимущества
- Сравнение с альтернативными решениями (Apache NiFi, Prefect)
- Обоснование выбора Apache Airflow 3.0.4 для проекта

#### 1.4.2. Выбор систем хранения данных

- **PostgreSQL**: выбор для реляционных данных
- **MinIO**: выбор для промежуточного хранения файлов (S3-совместимое хранилище)
- **Apache Superset**: инструмент для визуализации данных
- Обоснование выбора каждой системы

---

## 2. ПРОЕКТИРОВАНИЕ И РЕАЛИЗАЦИЯ СИСТЕМЫ

**Объем: 15-18 страниц**

### 2.1. Архитектура системы и проектирование ETL-пайплайна

- Общая архитектура решения
- Компоненты системы:
  - Apache Airflow (Scheduler, DAG Processor, API Server)
  - PostgreSQL (основное хранилище данных)
  - MinIO (промежуточное хранилище файлов)
  - Selenium Grid (автоматизация браузера)
  - Apache Superset (визуализация)
- Поток данных от источника до визуализации:
  1. Аутентификация через Selenium
  2. Извлечение данных через API
  3. Сохранение в MinIO
  4. Загрузка в PostgreSQL
  5. Визуализация в Superset
- Проектирование этапов ETL-пайплайна

### 2.2. Проектирование схемы базы данных

- Модель данных для категорий товаров:
  - Таблица `category_l1_l2` (иерархия L1/L2)
  - Таблица `category_l3` (категории L3)
- Модель данных для аналитики продаж:
  - Таблица `category_info_days` (аналитика по категориям)
  - Таблица `info_subjects_days` (аналитика по товарам)
- Модель данных для товаров и сезонности:
  - Таблица `info_subjects` (информация о товарах)
  - Таблица `season_ratio` (коэффициенты сезонности)
- ER-диаграмма и связи между таблицами

### 2.3. Развертывание инфраструктуры и настройка компонентов

- Docker Compose конфигурация для развертывания всех сервисов
- Настройка Apache Airflow:
  - Конфигурация airflow.cfg
  - Настройка подключений к БД
- Настройка PostgreSQL:
  - Инициализация схемы `sales`
  - Создание таблиц
- Настройка MinIO:
  - Создание bucket `airflow-bucket`
  - Настройка прав доступа

### 2.4. Реализация модуля аутентификации

- Интеграция с Selenium Grid для автоматизации браузера
- Функция `get_cookies()`: автоматический вход в систему
- Извлечение cookies из браузера
- Передача cookies в HTTP-запросах к API
- Обработка ошибок аутентификации

### 2.5. Реализация DAG-ов для сбора данных

- **get_l1_l2_dag**: сбор иерархии категорий L1/L2
  - Функция `get_l1_l2()` для запроса данных
  - Обработка JSON-ответов и нормализация с pandas
  - Сохранение в MinIO

- **get_l3_dag**: сбор категорий L3
  - Функция `get_l3()` для запроса данных по каждой категории L2
  - Обработка вложенных структур данных

- **get_info_30_days_async_dag**: сбор аналитики продаж по категориям
  - Асинхронная обработка множества категорий L3
  - Функция `get_info_30_async_days()` для параллельных запросов
  - Обработка трендов и метрик продаж

- **get_info_subjects_30_days_async_dag**: сбор данных о товарах
  - Получение рейтинга товаров
  - Сбор аналитики по товарам за 30 дней

- **get_season_ratio_async_dag**: сбор коэффициентов сезонности
  - Запрос коэффициентов для товаров
  - Обработка временных рядов

### 2.6. Реализация асинхронной обработки и загрузки данных

- Использование `aiohttp` для параллельных HTTP-запросов
- Обработка rate limiting (ошибка 429) с экспоненциальным backoff
- Использование семафоров для контроля параллельности
- Сохранение промежуточных данных в MinIO (CSV файлы)
- Загрузка данных в PostgreSQL:
  - Функция `loading_data_to_postgres_dag`
  - Чтение CSV из MinIO
  - Преобразование данных с pandas
  - Загрузка через SQLAlchemy
- Обработка ошибок и валидация данных
- Настройка визуализации в Apache Superset:
  - Подключение к PostgreSQL
  - Создание дашбордов для анализа продаж

---

## 3. ТЕСТИРОВАНИЕ И РЕЗУЛЬТАТЫ

**Объем: 3-4 страницы**

### 3.1. Тестирование функциональности системы

- Тестирование модуля аутентификации:
  - Проверка получения cookies
  - Проверка валидности cookies для API-запросов
- Тестирование DAG-ов:
  - Проверка корректности извлечения данных
  - Проверка обработки и нормализации данных
  - Проверка загрузки в хранилища
- Проверка целостности данных в PostgreSQL

### 3.2. Анализ производительности и оптимизация

- Сравнение синхронной и асинхронной обработки:
  - Метрики времени выполнения
  - Анализ производительности асинхронных DAG-ов
- Оптимизация системы:
  - Оптимизация запросов к API (обработка rate limiting)
  - Оптимизация работы с базой данных
  - Улучшение обработки ошибок
- Результаты тестирования и выводы

---

## ЗАКЛЮЧЕНИЕ

**Объем: 2-3 страницы**

- Краткое резюме выполненной работы
- Достигнутые цели и задачи
- Основные результаты исследования
- Практическая значимость работы
- Возможности дальнейшего развития системы
- Выводы по работе

---

## СПИСОК ИСПОЛЬЗОВАННЫХ ИСТОЧНИКОВ

**Объем: 1 страница**

- Книги и учебные пособия
- Научные статьи
- Официальная документация технологий:
  - Apache Airflow Documentation
  - PostgreSQL Documentation
  - Selenium Documentation
  - Docker Documentation
- Онлайн-ресурсы и блоги

---

## ПРИЛОЖЕНИЯ

**Объем: 2-3 страницы**

### Приложение А. Конфигурационные файлы проекта

- docker-compose.yaml (фрагмент)
- airflow.cfg (основные настройки)
- requirements.txt

### Приложение Б. Примеры кода DAG-ов

- get_l1_l2_dag.py (основной код)
- get_info_30_days_async_dag.py (фрагмент с асинхронной обработкой)
- loading_data_to_postgres_dag.py (фрагмент загрузки данных)

### Приложение В. Схема базы данных

- ER-диаграмма таблиц схемы `sales`
- Описание таблиц и их полей
- SQL-скрипт создания схемы (фрагмент)

---

**Рекомендации по оформлению:**
- Шрифт: Times New Roman, 14pt
- Межстрочный интервал: 1.5
- Поля: левое 30мм, остальные 20мм
- Нумерация страниц: внизу по центру
- Ссылки на источники в квадратных скобках [1]

**Распределение объема по разделам:**
- Введение: 2-3 стр. (6-9%)
- Глава 1: 8-10 стр. (24-30%)
- Глава 2: 15-18 стр. (45-54%)
- Глава 3: 3-4 стр. (9-12%)
- Заключение: 2-3 стр. (6-9%)
- Список литературы: 1 стр. (3%)
- Приложения: 2-3 стр. (6-9%)

**Итого: 30-35 страниц**
